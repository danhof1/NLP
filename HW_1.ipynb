{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E0N2ywIzcp-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc35919c-9407-440e-da88-554d50002637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q nltk\n",
        "import nltk\n",
        "nltk.download(\"book\")\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1"
      ],
      "metadata": {
        "id": "QEcQWta_GBjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part A\n",
        "###Tokenize all sentences (use sent_tokenize), and extract all tokens from each sentence (use word_tokenize). Also lowercase all tokens. Do not use other processing steps for now. Keep the tokens in each sentence in a list (you should have a list of lists of tokens). Display the number of sentences you found."
      ],
      "metadata": {
        "id": "UdPLmm4HExYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/alice.txt\", 'r') as f:\n",
        "  text = f.read()  # Read the full text and store as a string"
      ],
      "metadata": {
        "id": "dRwMBylTL_5C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Concatenate all tokens from the list of lists at (A) in a single list of tokens. Display the number of tokens."
      ],
      "metadata": {
        "id": "8-ZaqM0RE59i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(text) # tokenizes all sentences in text\n",
        "print(f\"Number of sentences found: {len(sentences)}\") # prints number of sentences\n",
        "tokens_sentence = [nltk.word_tokenize(sentence.lower()) for sentence in sentences] # tokenzies the words for each sentence and lowercase the tokens\n",
        "all_tokens = sum(tokens_sentence, []) # concatenates lists of tokens from tokens_sentence into one list\n",
        "print(f\"Number of tokens found: {len(all_tokens)}\") # prints number of tokens\n",
        "print(all_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOkkqLKCPxe8",
        "outputId": "e1dc8f64-ddee-46a8-f458-75b2b5142f4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences found: 1616\n",
            "Number of tokens found: 34399\n",
            "['alice', \"'s\", 'adventures', 'in', 'wonderland', 'alice', \"'s\", 'adventures', 'in', 'wonderland', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', '3.0', 'chapter', 'i', 'down']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####(2) Write a function that stores the unique tokens and their frequency in a dictionary (write your own function).\n",
        "####The function needs to return a dictionary with all tokens as keys and frequencies as values. Display the number of unique tokens.\n"
      ],
      "metadata": {
        "id": "cL3K5p-FFEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def token_dictionary(tokens):\n",
        "  freq_dict = {} # dictionary that stores frequency of each token\n",
        "  for token in tokens:\n",
        "    if token in freq_dict:\n",
        "      freq_dict[token] += 1 # if token in dictionary increment count by 1\n",
        "    else:\n",
        "      freq_dict[token] = 1 # if token not in dictionary add it\n",
        "  return freq_dict"
      ],
      "metadata": {
        "id": "S8v07CF_rUg2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_freq_dict = token_dictionary(all_tokens) # stores dictionary of tokens\n",
        "print(f\"Number of unique tokens: {len(token_freq_dict)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnR9cbs3tcKN",
        "outputId": "9992cb34-411e-43d3-9a26-438ec587e524"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 2650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####(3) Write a function that sorts a dictionary by values from largest to lowest.\n",
        "####In this case, the values are frequencies (see example at the end of notebook_intro_nlp.ipynb in class repository).\n",
        "####Sort the dictionary at A.2. and display the 10 most and the 10 least frequent terms and their frequencies.\n"
      ],
      "metadata": {
        "id": "cRRPTxVKFVXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_by_frequency(freq_dict):\n",
        "    sorted_freq = sorted(freq_dict.items(), key=lambda item: item[1], reverse=True) # sorts the dictionary by values in descending order\n",
        "    return sorted_freq"
      ],
      "metadata": {
        "id": "OQ44Hlkky4n2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_token_freq = sort_by_frequency(token_freq_dict) #  sorts the token frequency dictionary\n",
        "\n",
        "print(\"10 most frequent terms:\")\n",
        "for token, freq in sorted_token_freq[:10]: # starts listing the top 10 frequency tokens\n",
        "    print(f\"{token}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent terms:\")\n",
        "for token, freq in sorted_token_freq[-10:]: # starts from the end of the sorted dictionary and displays the least 10 frequency tokens\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "id": "plHyGG0pziOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a04db28-4c79-418f-b21e-ff4ed4ccf954"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 most frequent terms:\n",
            ",: 2418\n",
            "the: 1639\n",
            "`: 1109\n",
            "': 1102\n",
            ".: 970\n",
            "and: 866\n",
            "to: 725\n",
            "a: 631\n",
            "it: 591\n",
            "she: 553\n",
            "\n",
            "10 least frequent terms:\n",
            "riper: 1\n",
            "years: 1\n",
            "loving: 1\n",
            "childhood: 1\n",
            "gather: 1\n",
            "sorrows: 1\n",
            "joys: 1\n",
            "remembering: 1\n",
            "child-life: 1\n",
            "happy: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####(4) Remove all punctuation marks from the dictionary at (3).\n",
        "####Display the number of unique tokens, the 10 most and 10 least frequent terms and their frequencies."
      ],
      "metadata": {
        "id": "F-USgY24FdrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_punctuation(tokens): # function to remove punctuation from tokens\n",
        "    return [re.sub(r'[^\\w\\s]', '', token) for token in tokens if re.sub(r'[^\\w\\s]', '', token)]"
      ],
      "metadata": {
        "id": "37HxGFTU8UDq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = remove_punctuation(all_tokens)\n",
        "token_freq_dict = token_dictionary(all_tokens) # stores dictionary of tokens without punctuation\n",
        "\n",
        "print(f\"Number of unique tokens (without punctuation marks): {len(token_freq_dict)}\")\n",
        "\n",
        "sorted_token_freq = sort_by_frequency(token_freq_dict) # sorts the token frequency dictionary\n",
        "\n",
        "print(\"10 most frequent terms (without punctuation marks):\")\n",
        "for token, freq in sorted_token_freq[:10]: # starts listing the top 10 frequency tokens\n",
        "    print(f\"{token}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent terms (without punctuation marks):\")\n",
        "for token, freq in sorted_token_freq[-10:]: # starts from the end of the sorted dictionary and displays the least 10 frequency tokens\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "id": "hbM97LM-RJpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31df82f-22b7-4ed2-b578-d66ba09917fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens (without punctuation marks): 2615\n",
            "10 most frequent terms (without punctuation marks):\n",
            "the: 1639\n",
            "and: 866\n",
            "to: 725\n",
            "a: 631\n",
            "it: 591\n",
            "she: 553\n",
            "i: 525\n",
            "of: 511\n",
            "said: 462\n",
            "you: 406\n",
            "\n",
            "10 least frequent terms (without punctuation marks):\n",
            "riper: 1\n",
            "years: 1\n",
            "loving: 1\n",
            "childhood: 1\n",
            "gather: 1\n",
            "sorrows: 1\n",
            "joys: 1\n",
            "remembering: 1\n",
            "childlife: 1\n",
            "happy: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####(5) Remove all stop words from the dictionary at (4).\n",
        "####Display the number of unique tokens, the 10 most and 10 least frequent terms and their frequencies."
      ],
      "metadata": {
        "id": "-wkX3n0TFi_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "def remove_stopwords(tokens): # function that removes stopwords\n",
        "    stop_words = set(stopwords.words('english')) # sets all words that are considered stop words in strings\n",
        "    return [token for token in tokens if token not in stop_words]"
      ],
      "metadata": {
        "id": "ak2Jbpa7XnXS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = remove_stopwords(all_tokens)\n",
        "token_freq_dict = token_dictionary(all_tokens) # stores dictionary of tokens without punctuation\n",
        "\n",
        "print(f\"Number of unique tokens (without stopwords): {len(token_freq_dict)}\")\n",
        "\n",
        "sorted_token_freq = sort_by_frequency(token_freq_dict) # sorts the token frequency dictionary\n",
        "\n",
        "print(\"10 most frequent terms (without stopwords):\")\n",
        "for token, freq in sorted_token_freq[:10]: # starts listing the top 10 frequency tokens\n",
        "    print(f\"{token}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent terms (without stopwords):\")\n",
        "for token, freq in sorted_token_freq[-10:]: # starts from the end of the sorted dictionary and displays the least 10 frequency tokens\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "id": "fwCRsl4XYNfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bcdbdf-ee10-408b-ecc9-911cd25c0e22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens (without stopwords): 2483\n",
            "10 most frequent terms (without stopwords):\n",
            "said: 462\n",
            "alice: 399\n",
            "nt: 208\n",
            "little: 128\n",
            "one: 104\n",
            "would: 96\n",
            "know: 88\n",
            "could: 86\n",
            "like: 85\n",
            "went: 83\n",
            "\n",
            "10 least frequent terms (without stopwords):\n",
            "riper: 1\n",
            "years: 1\n",
            "loving: 1\n",
            "childhood: 1\n",
            "gather: 1\n",
            "sorrows: 1\n",
            "joys: 1\n",
            "remembering: 1\n",
            "childlife: 1\n",
            "happy: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####(6) Stem all remaining words.\n",
        "####Use the PorterStemmer and rebuild the dictionary so that identical stemmed words appear only once and their frequencies are summed.\n",
        "####Display the number of unique tokens, the 10 most and 10 least frequent terms and their frequencies."
      ],
      "metadata": {
        "id": "lWsNuCjYFr24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer() # initalizes the Porter Stemmer\n",
        "def stem_tokens(tokens): # function to stem tokens using Porter Stemmer\n",
        "    return [stemmer.stem(token) for token in tokens]"
      ],
      "metadata": {
        "id": "sPUh8KLmchAA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = stem_tokens(all_tokens)\n",
        "token_freq_dict = token_dictionary(all_tokens) # stores dictionary of tokens without punctuation\n",
        "\n",
        "print(f\"Number of unique tokens (without stopwords): {len(token_freq_dict)}\")\n",
        "\n",
        "sorted_token_freq = sort_by_frequency(token_freq_dict) # sorts the token frequency dictionary\n",
        "\n",
        "print(\"10 most frequent terms (without stopwords):\")\n",
        "for token, freq in sorted_token_freq[:10]: # starts listing the top 10 frequency tokens\n",
        "    print(f\"{token}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent terms (without stopwords):\")\n",
        "for token, freq in sorted_token_freq[-10:]: # starts from the end of the sorted dictionary and displays the least 10 frequency tokens\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "id": "ZgcHvMoWc2qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e190bf-9226-4e67-b49a-19a856528cdf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens (without stopwords): 1899\n",
            "10 most frequent terms (without stopwords):\n",
            "said: 462\n",
            "alic: 399\n",
            "nt: 208\n",
            "littl: 128\n",
            "one: 105\n",
            "look: 104\n",
            "like: 97\n",
            "would: 96\n",
            "know: 92\n",
            "could: 86\n",
            "\n",
            "10 least frequent terms (without stopwords):\n",
            "farmyard: 1\n",
            "cattl: 1\n",
            "lastli: 1\n",
            "aftertim: 1\n",
            "riper: 1\n",
            "childhood: 1\n",
            "gather: 1\n",
            "joy: 1\n",
            "childlif: 1\n",
            "happi: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part B\n",
        "###Use the list of sentence tokens you obtained at 1.A\n"
      ],
      "metadata": {
        "id": "Zue3kbwnGRJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Extract the POS (Parts Of Speech) of all tokens in each sentence (use pos_tag from nltk library).You will obtain a list of tuples for each sentence. All sentence lists will be inside a list. A tuple will contain the token and its POS. Display the tuples in a few sentences in the middle of the book."
      ],
      "metadata": {
        "id": "fWRki9g4GYCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "def extract_pos(tokens_sentence): # function to extract POS of all tokens in each sentence\n",
        "  pos_sentences = [pos_tag(tokens) for tokens in tokens_sentence] # gets the POS of each sentence\n",
        "  return pos_sentences"
      ],
      "metadata": {
        "id": "c3dfFO0TWyrw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_sentences = extract_pos(tokens_sentence) # extracts POS of all tokens for each sentence\n",
        "middle_index = len(pos_sentences) // 2 # finds the middle index of the text\n",
        "print(\"\\nPOS for sentences in the middle of the book: \\n\")\n",
        "\n",
        "for sentence_pos in pos_sentences [middle_index:middle_index + 5]: # displays the 5 sentences in the middle of the book and their POS\n",
        "  print(sentence_pos)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjUvgdCObtkd",
        "outputId": "22fab2fa-f4ad-4c32-b586-f7aaa80645db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS for sentences in the middle of the book: \n",
            "\n",
            "[('`', '``'), ('i', 'NN'), ('dare', 'NN'), ('say', 'VBP'), ('you', 'PRP'), ('never', 'RB'), ('even', 'RB'), ('spoke', 'VBD'), ('to', 'TO'), ('time', 'NN'), ('!', '.'), (\"'\", \"''\")]\n",
            "\n",
            "\n",
            "[('`', '``'), ('perhaps', 'RB'), ('not', 'RB'), (',', ','), (\"'\", \"''\"), ('alice', 'RB'), ('cautiously', 'RB'), ('replied', 'VBN'), (':', ':'), ('`', '``'), ('but', 'CC'), ('i', 'NN'), ('know', 'VBP'), ('i', 'RB'), ('have', 'VBP'), ('to', 'TO'), ('beat', 'VB'), ('time', 'NN'), ('when', 'WRB'), ('i', 'NN'), ('learn', 'VBP'), ('music', 'NN'), ('.', '.'), (\"'\", \"''\")]\n",
            "\n",
            "\n",
            "[('`', '``'), ('ah', 'NN'), ('!', '.')]\n",
            "\n",
            "\n",
            "[('that', 'DT'), ('accounts', 'VBZ'), ('for', 'IN'), ('it', 'PRP'), (',', ','), (\"'\", \"''\"), ('said', 'VBD'), ('the', 'DT'), ('hatter', 'NN'), ('.', '.')]\n",
            "\n",
            "\n",
            "[('`', '``'), ('he', 'PRP'), ('wo', 'MD'), (\"n't\", 'RB'), ('stand', 'VB'), ('beating', 'NN'), ('.', '.')]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) Lemmatize the nouns, verbs, adjectives, and adverbs in each sentence using nltk (see notebook_nlp_intro.ipynb): noun tokens have POS that starts with N, verb tokens, have POS that starts with V, adverb tokens have POS that starts with R, and adjective tokens have POS that starts with J. You will have to change the original POS into a letter 'n' for nouns, 'v' for verbs, 'a' for adjectives, and 'r' for adverbs. Do not remove any other tokens for now. Display the tuples in a few sentences in the middle of the book."
      ],
      "metadata": {
        "id": "agBavm8TiTNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() # initializes the WordNet Lemmatizer\n",
        "\n",
        "def get_wordnet(tag): # function that maps nltk POS tags to wordnet POS tags\n",
        "  if tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  if tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  if tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  if tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "lGlhxSn9f49Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_sentence(pos_sentences):\n",
        "  lemmatized_sentence = []\n",
        "  for word, pos in pos_sentences:\n",
        "    wordnet_pos = get_wordnet(pos) # gets the wordnet POS\n",
        "    if wordnet_pos: # if the word POS is a noun, verb, adjective or adverb\n",
        "      lemmatized_word = lemmatizer.lemmatize(word, wordnet_pos)\n",
        "    else:\n",
        "      lemmatized_word = word # keeps the word as is if not Noun, Verb, Adjective, or Adverb\n",
        "    lemmatized_sentence.append((lemmatized_word, pos)) # keeps the original POS\n",
        "  return lemmatized_sentence"
      ],
      "metadata": {
        "id": "TexdCBj4hD0Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_sentences = [lemmatize_sentence(sentence) for sentence in pos_sentences] # lemmatizes each sentence\n",
        "\n",
        "middle_index = len(lemmatized_sentences) // 2 # finds the middle index of the text\n",
        "print(\"\\n Lemmatized sentences in the middle of the book: \\n\")\n",
        "\n",
        "for sentence in lemmatized_sentences [middle_index:middle_index + 5]: # displays the 5 sentences in the middle of the book and their POS\n",
        "  print(sentence)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJGIZ9hZjlFW",
        "outputId": "bffc714b-7f38-497d-d16a-2e845d6ed55a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Lemmatized sentences in the middle of the book: \n",
            "\n",
            "[('`', '``'), ('i', 'NN'), ('dare', 'NN'), ('say', 'VBP'), ('you', 'PRP'), ('never', 'RB'), ('even', 'RB'), ('speak', 'VBD'), ('to', 'TO'), ('time', 'NN'), ('!', '.'), (\"'\", \"''\")]\n",
            "\n",
            "\n",
            "[('`', '``'), ('perhaps', 'RB'), ('not', 'RB'), (',', ','), (\"'\", \"''\"), ('alice', 'RB'), ('cautiously', 'RB'), ('reply', 'VBN'), (':', ':'), ('`', '``'), ('but', 'CC'), ('i', 'NN'), ('know', 'VBP'), ('i', 'RB'), ('have', 'VBP'), ('to', 'TO'), ('beat', 'VB'), ('time', 'NN'), ('when', 'WRB'), ('i', 'NN'), ('learn', 'VBP'), ('music', 'NN'), ('.', '.'), (\"'\", \"''\")]\n",
            "\n",
            "\n",
            "[('`', '``'), ('ah', 'NN'), ('!', '.')]\n",
            "\n",
            "\n",
            "[('that', 'DT'), ('account', 'VBZ'), ('for', 'IN'), ('it', 'PRP'), (',', ','), (\"'\", \"''\"), ('say', 'VBD'), ('the', 'DT'), ('hatter', 'NN'), ('.', '.')]\n",
            "\n",
            "\n",
            "[('`', '``'), ('he', 'PRP'), ('wo', 'MD'), (\"n't\", 'RB'), ('stand', 'VB'), ('beating', 'NN'), ('.', '.')]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) Write a function that stores the unique tuples (token,POS) and their frequency in a dictionary (write your own function). The function needs to return a dictionary with all unique tuples as keys and frequencies as values. Display the number of unique tuples."
      ],
      "metadata": {
        "id": "yN1RxK3_ifM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tuple_frequency_dict(pos_sentences):\n",
        "  freq_dict = {} # stores tuple frequency\n",
        "  for sentence in pos_sentences: # iterates over POS sentences\n",
        "    for token_pos_tuple in sentence: # iterates over each tuple in the sentence\n",
        "      if token_pos_tuple in freq_dict:\n",
        "        freq_dict[token_pos_tuple] +=1 # increments the count if the tuple is already in the dictionary\n",
        "      else:\n",
        "        freq_dict[token_pos_tuple] = 1 # adds the tuple to the dictionary with a count of 1\n",
        "  return freq_dict"
      ],
      "metadata": {
        "id": "yL4s1jsqpKR6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuple_freq_dict = tuple_frequency_dict(lemmatized_sentences)\n",
        "print(f\"Number of unique tuples: {len(tuple_freq_dict)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xWOfh2Rsj6i",
        "outputId": "46924e64-744a-483b-9570-343cdae5655f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tuples: 3244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (4) Sort the dictionary by frequencies from largest to smallest tuple (see example at the end of notebook_intro_nlp.ipynb in class repository). Display the 10 most and the 10 least frequent tuples and their frequencies."
      ],
      "metadata": {
        "id": "mhGyMdnTiqas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_tuple_freq = sort_by_frequency(tuple_freq_dict) #  sorts the tuple frequency dictionary\n",
        "\n",
        "print(\"10 most frequent tuples:\")\n",
        "for token_pos_tuple, freq in sorted_tuple_freq[:10]: # starts listing the top 10 tuple frequency\n",
        "    print(f\"{token_pos_tuple}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent tuples:\")\n",
        "for token_pos_tuple, freq in sorted_tuple_freq[-10:]: # starts from the end of the sorted dictionary and displays the least 10 tuple frequency tokens\n",
        "    print(f\"{token_pos_tuple}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P01dF_uuvIvA",
        "outputId": "dcfe402f-b83d-4518-80d9-19641d0ca28a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 most frequent tuples:\n",
            "(',', ','): 2418\n",
            "('the', 'DT'): 1639\n",
            "('`', '``'): 1109\n",
            "(\"'\", \"''\"): 1061\n",
            "('.', '.'): 970\n",
            "('and', 'CC'): 866\n",
            "('to', 'TO'): 725\n",
            "('a', 'DT'): 631\n",
            "('it', 'PRP'): 591\n",
            "('she', 'PRP'): 553\n",
            "\n",
            "10 least frequent tuples:\n",
            "('love', 'VBG'): 1\n",
            "('childhood', 'NN'): 1\n",
            "('gather', 'VB'): 1\n",
            "('bright', 'VBN'): 1\n",
            "('ago', 'NN'): 1\n",
            "('sorrow', 'NNS'): 1\n",
            "('joy', 'NNS'): 1\n",
            "('remember', 'VBG'): 1\n",
            "('child-life', 'NN'): 1\n",
            "('happy', 'JJ'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (5) Remove all tuples with empty POS (it should remove all punctuation marks). Display the number of unique tuples and the 10 most and 10 least frequent tuples and their frequencies."
      ],
      "metadata": {
        "id": "UbeHLKXQiy95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punct_tuples(freq_dict):\n",
        "  return {key: value for key, value in freq_dict.items() if key[1].isalpha()} # only keeps tuples that are alphabetical, so punctuation marks are removed\n",
        "\n",
        "tuple_freq_dict_npunct = remove_punct_tuples(tuple_freq_dict) # removes punctuation marks from tuples\n",
        "print(f\"Number of unique tuples with no punctuation marks: {len(tuple_freq_dict_npunct)}\")\n",
        "\n",
        "sorted_tuple_freq_npunct = sort_by_frequency(tuple_freq_dict_npunct) # sorts the updated dictionary\n",
        "\n",
        "print(\"10 most frequent tuples with no punctuation marks:\")\n",
        "for token_pos_tuple, freq in sorted_tuple_freq_npunct[:10]: # starts listing the top 10 tuple frequency with no punctuation\n",
        "    print(f\"{token_pos_tuple}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent tuples with no punctuation marks:\")\n",
        "for token_pos_tuple, freq in sorted_tuple_freq_npunct[-10:]: # starts from the end of the sorted dictionary and displays the least 10 tuple frequency tokens with no punctuation\n",
        "    print(f\"{token_pos_tuple}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ACG2BEQxbR-",
        "outputId": "63fa3910-2951-4d20-fbf4-35258a56caff"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tuples with no punctuation marks: 3221\n",
            "10 most frequent tuples with no punctuation marks:\n",
            "('the', 'DT'): 1639\n",
            "('and', 'CC'): 866\n",
            "('to', 'TO'): 725\n",
            "('a', 'DT'): 631\n",
            "('it', 'PRP'): 591\n",
            "('she', 'PRP'): 553\n",
            "('of', 'IN'): 511\n",
            "('say', 'VBD'): 462\n",
            "('be', 'VBD'): 454\n",
            "('you', 'PRP'): 406\n",
            "\n",
            "10 least frequent tuples with no punctuation marks:\n",
            "('love', 'VBG'): 1\n",
            "('childhood', 'NN'): 1\n",
            "('gather', 'VB'): 1\n",
            "('bright', 'VBN'): 1\n",
            "('ago', 'NN'): 1\n",
            "('sorrow', 'NNS'): 1\n",
            "('joy', 'NNS'): 1\n",
            "('remember', 'VBG'): 1\n",
            "('child-life', 'NN'): 1\n",
            "('happy', 'JJ'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (6) Remove all tuples that are NOT nouns, adjectives, adverbs, or verbs. Display the number of unique tuples and the 10 most and 10 least frequent tuples and their frequencies."
      ],
      "metadata": {
        "id": "ekvos7b4i-rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_naav_tuples(freq_dict): # function that remove tuples that are not Nouns, Adjectives, Adverbs, Verbs\n",
        "  return {key: value for key, value in freq_dict.items() if key[1] == \"NN\" or key[1] == \"JJ\" or key[1] == \"RB\" or key[1] == \"VB\"} # only keeps Nouns, Adjectives, Adverbs, Verbs\n",
        "\n",
        "tuple_freq_dict_naav = remove_naav_tuples(tuple_freq_dict_npunct) # removes tuples that are not Nouns, Adjectives, Adverbs, or Verbs\n",
        "print(f\"Number of unique tuples with only Nouns, Adj, Adverbs, and Verbs: {len(tuple_freq_dict_naav)}\")\n",
        "\n",
        "sorted_tuple_freq_naav = sort_by_frequency(tuple_freq_dict_naav) # sorts the updated dictionary\n",
        "\n",
        "print(\"10 most frequent tuples with only Nouns, Adj, Adverbs, and Verbs:\")\n",
        "for token_pos_tuple, freq in sorted_tuple_freq_naav[:10]: # starts listing the top 10 tuple frequency with only Nouns, Adjectives, Adverbs, Verbs\n",
        "    print(f\"{token_pos_tuple}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent tuples with only Nouns, Adj, Adverbs, and Verbs:\")\n",
        "for token_pos_tuple, freq in sorted_tuple_freq_naav[-10:]: # starts from the end of the sorted dictionary and displays the least 10 tuple frequency tokens with only Nouns, Adjectives, Adverbs, Verbs\n",
        "    print(f\"{token_pos_tuple}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CGV7oVW1szu",
        "outputId": "e53eaf83-e062-430e-bea5-ab565f1aaa75"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tuples with only Nouns, Adj, Adverbs, and Verbs: 1888\n",
            "10 most frequent tuples with only Nouns, Adj, Adverbs, and Verbs:\n",
            "('alice', 'NN'): 311\n",
            "('i', 'NN'): 276\n",
            "(\"n't\", 'RB'): 208\n",
            "('be', 'VB'): 148\n",
            "('not', 'RB'): 146\n",
            "('very', 'RB'): 144\n",
            "('little', 'JJ'): 127\n",
            "('so', 'RB'): 118\n",
            "('i', 'JJ'): 98\n",
            "('then', 'RB'): 94\n",
            "\n",
            "10 least frequent tuples with only Nouns, Adj, Adverbs, and Verbs:\n",
            "(\"turtle's\", 'NN'): 1\n",
            "('lastly', 'RB'): 1\n",
            "('after-time', 'JJ'): 1\n",
            "('grown', 'JJ'): 1\n",
            "('riper', 'NN'): 1\n",
            "('childhood', 'NN'): 1\n",
            "('gather', 'VB'): 1\n",
            "('ago', 'NN'): 1\n",
            "('child-life', 'NN'): 1\n",
            "('happy', 'JJ'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(C) - As I added more processing steps to the dictionary of words from the Alice in Wonderland text, the number of unique tokens goes down for when I went from A.1 to A.6 and B.1 to B.6. Immediately, the steps through 1B end up giving a more informative dictionary, since it gives the part of speech of the word on top of the word frequency in the text. In 1A.2, I had 2650 unique tokens and in 1B.3 I had 3244 unique tokens, and although the number of tokens does decrease by a great amount, the steps through 1.B help lead to a better list of words that is shorter and much more informative than doing the steps through 1.A. This is due to the fact that the stop words removal with stemming  ends up making some words hard for identifying what part of speech they are, thus making it harder to  identify the word in general, whereas the POS removal with lemmatization is much more efficient since the final count of unique tokens is 1888 for 1.B which is less than the count in 1.A and it also contains more important terms since its only the nouns, adjectives, adverbs, and verbs from the text."
      ],
      "metadata": {
        "id": "-aw50dSKzGZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (D) Propose at least one additional processing step for each (A) and (B) that you think will improve the resulting list of tokens - shorter and more informative of the subject matter. Implement it, show its effect and comment on the results you obtained."
      ],
      "metadata": {
        "id": "gq3miN_fjjfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_tokens(freq_dict, min_length=3):\n",
        "    return {token: freq for token, freq in freq_dict.items() if len(token) >= min_length} # removes words that are shorter than 2 characters\n",
        "\n",
        "token_freq_dict_st = remove_short_tokens(token_freq_dict, min_length=3) # removes short tokens\n",
        "\n",
        "sorted_token_freq_st = sort_by_frequency(token_freq_dict_st) # sorts and displays the updated token frequency dictionary\n",
        "\n",
        "print(f\"Number of unique tokens after filtering short tokens: {len(token_freq_dict_st)}\")\n",
        "\n",
        "print(\"10 most frequent tokens (filtered by length):\") # displays the 10 most frequent tokens after removing short tokens\n",
        "for token, freq in sorted_token_freq_st[:10]:\n",
        "    print(f\"{token}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent tokens (filtered by length):\") # displays 10 least frequent tokens after removing short tokens\n",
        "for token, freq in sorted_token_freq_st[-10:]:\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ53KApToKlg",
        "outputId": "b73ef92e-762b-41e2-b0a0-6a24a6258210"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens after filtering short tokens: 1870\n",
            "10 most frequent tokens (filtered by length):\n",
            "said: 462\n",
            "alic: 399\n",
            "littl: 128\n",
            "one: 105\n",
            "look: 104\n",
            "like: 97\n",
            "would: 96\n",
            "know: 92\n",
            "could: 86\n",
            "went: 83\n",
            "\n",
            "10 least frequent tokens (filtered by length):\n",
            "farmyard: 1\n",
            "cattl: 1\n",
            "lastli: 1\n",
            "aftertim: 1\n",
            "riper: 1\n",
            "childhood: 1\n",
            "gather: 1\n",
            "joy: 1\n",
            "childlif: 1\n",
            "happi: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_low_frequency_tuples(freq_dict, min_freq=2): # function removes tuples with low frequencies like 2 or less\n",
        "    return {key: value for key, value in freq_dict.items() if value > min_freq}\n",
        "\n",
        "tuple_freq_dict_naav = remove_low_frequency_tuples(tuple_freq_dict_naav, min_freq=2) # removes low-frequency tuples\n",
        "\n",
        "sorted_tuple_freq_naav = sort_by_frequency(tuple_freq_dict_naav) # sorts and displays the updated tuple frequency dictionary\n",
        "\n",
        "print(f\"Number of unique tuples after low-frequency tuple removal: {len(sorted_tuple_freq_naav)}\")\n",
        "\n",
        "print(\"10 most frequent tuples after low-frequency tuple removal:\") # displays the 10 most frequent tuples after low-frequency tuple removal\n",
        "for token_pos_tuple, freq in sorted_tuple_freq_naav[:10]:\n",
        "    print(f\"{token_pos_tuple}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent tuples after low-frequency tuple removal:\") # displays the 10 least frequent tuples after low-frequency tuple removal\n",
        "for token_pos_tuple, freq in sorted_tuple_freq_naav[-10:]:\n",
        "    print(f\"{token_pos_tuple}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLPT-_64uEYT",
        "outputId": "7c23407f-2988-4a87-b39a-7f6433c10357"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tuples after low-frequency tuple removal: 645\n",
            "10 most frequent tuples after low-frequency tuple removal:\n",
            "('alice', 'NN'): 311\n",
            "('i', 'NN'): 276\n",
            "(\"n't\", 'RB'): 208\n",
            "('be', 'VB'): 148\n",
            "('not', 'RB'): 146\n",
            "('very', 'RB'): 144\n",
            "('little', 'JJ'): 127\n",
            "('so', 'RB'): 118\n",
            "('i', 'JJ'): 98\n",
            "('then', 'RB'): 94\n",
            "\n",
            "10 least frequent tuples after low-frequency tuple removal:\n",
            "('quadrille', 'NN'): 3\n",
            "('figure', 'NN'): 3\n",
            "('snail', 'NN'): 3\n",
            "('owl', 'NN'): 3\n",
            "('panther', 'NN'): 3\n",
            "('trumpet', 'NN'): 3\n",
            "('consider', 'VB'): 3\n",
            "('teacup', 'NN'): 3\n",
            "('list', 'NN'): 3\n",
            "('swim', 'VB'): 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(D) - For steps A, I implemented an additional processing step that removes short tokens from the list of tokens, in which if the token only has a length of 2 characters then it will be removed. I did this since tokens that only have 1-2 characters are terms such as \"a\" or \"is\" and ultimately they are not that informative on the overall subject matter of the text \"Alice in Wonderland\". Once I implemented this, even though the count of unique tokens did not go down by that much, it still resulted in a more informative list of terms. For steps B, I implemented an additional processing step that removes tuples that have a very small frequency, in which it is a term that only shows up 1-2 times in the text. I did this since a term that only shows up 1-2 times in the text is most definitely not necessary to understand the overall subject matter. Once I implemented this, I got a very concise list of tuples where there were only 645 tuples, all with their respective parts of speech, so its a very short but informative list on the subject matter."
      ],
      "metadata": {
        "id": "s87xcrEQysGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "VhI4MiuKmkIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (A) Use spacy to process the file and to apply the processing steps (lowercase, remove stopwords and punctuation marks, lemmatize words). Display the number of unique tokens, the 10 most and 10 least frequent terms, and their frequencies (use the Counter object in Python)"
      ],
      "metadata": {
        "id": "2-5HXsLBmqeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIyhR2LdzzJU",
        "outputId": "759eda12-f3f0-4491-ab75-646cbb3c0d7e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc_processed = nlp(text.lower()) # processes the text and lowercases all characters\n",
        "\n",
        "tokens_spac = [token.lemma_ for token in doc_processed if not token.is_stop and not token.is_punct and token.lemma_.isalpha()] # creates a list to store the lemmatized tokens that also removes stopwords and punctuation marks\n",
        "\n",
        "token_frequency = Counter(tokens_spac) # token frequency is calculated\n",
        "\n",
        "print(f\"Number of unique tokens: {len(token_frequency)}\") # displays number of unique tokens\n",
        "\n",
        "sorted_token_freq = token_frequency.most_common() # sorts tokens by frequency\n",
        "\n",
        "print(\"10 most frequent terms:\") # displays the 10 most frequent terms\n",
        "for token, freq in sorted_token_freq[:10]:\n",
        "    print(f\"{token}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent terms:\") # displays the 10 least frequent terms\n",
        "for token, freq in sorted_token_freq[-10:]:\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4unFi7h_z0yI",
        "outputId": "1ca558ca-5848-49ab-d3a4-eb213571d8e7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 1765\n",
            "10 most frequent terms:\n",
            "say: 475\n",
            "alice: 398\n",
            "think: 129\n",
            "go: 129\n",
            "little: 127\n",
            "look: 105\n",
            "know: 103\n",
            "like: 92\n",
            "begin: 91\n",
            "come: 90\n",
            "\n",
            "10 least frequent terms:\n",
            "lowing: 1\n",
            "cattle: 1\n",
            "lastly: 1\n",
            "grown: 1\n",
            "woman: 1\n",
            "riper: 1\n",
            "childhood: 1\n",
            "gather: 1\n",
            "joy: 1\n",
            "happy: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(B) - Each processing step goes much faster and much more efficiently through using spacy, in which the code that is required to get the processing steps done is very concise sue to the fact that spacy has built in functions such as is_stop and is_punct to check if a word is a stopword or a punctuation mark. Compared to the results that I got from part 1 with nltk processing, spacy proved that it ends up making a shorter and informative list since it almost has 100 less tokens than both parts A and B with a count of 1750 unique tokens through spacy, and it definitely could be more informative if it was given more processing steps."
      ],
      "metadata": {
        "id": "5t3RBgDwyZwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (C) Read more about spacy and based on its features propose an additional processing step that will improve the final list of tokens. Implement it, and comment on the results you obtained."
      ],
      "metadata": {
        "id": "s8OR1pcFm0B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_spac2 = [] # creates a list that will sotre the lemmatized tokens that will only contain named entities and nouns, verbs, adjectives, adverbs\n",
        "\n",
        "for token in doc_processed:\n",
        "  if token.ent_type_ or token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]: # checks if token is named entitiy or noun, verb, adj, adverb\n",
        "    if not token.is_stop and not token.is_punct and token.lemma_.isalpha(): # checks if token is stopword, punctuation mark, or non-alphabetical\n",
        "      tokens_spac2.append(token.lemma_) # stores lemmatized token in list\n",
        "\n",
        "token_freq = Counter(tokens_spac2) # token frequency is calculated\n",
        "\n",
        "print(f\"Number of unique tokens with only named entities, nouns, adj, verbs, adverbs: {len(token_freq)}\") # displays number of unique tokens with only named entities and nouns, adj, verbs, adverbs\n",
        "\n",
        "sorted_token_freq2 = token_freq.most_common() # sorts tokens by frequency\n",
        "\n",
        "print(\"10 most frequent terms with only named entities, nouns, adj, verbs, adverbs:\") # displays the 10 most frequent terms\n",
        "for token, freq in sorted_token_freq2[:10]:\n",
        "    print(f\"{token}: {freq}\")\n",
        "\n",
        "print(\"\\n10 least frequent terms with only named entities, nouns, adj, verbs, adverbs:\") # displays the 10 least frequent terms\n",
        "for token, freq in sorted_token_freq2[-10:]:\n",
        "    print(f\"{token}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPeMsA8xXPag",
        "outputId": "431861b3-9d9a-425c-b74c-9246084d6491"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens with only named entities, nouns, adj, verbs, adverbs: 1708\n",
            "10 most frequent terms with only named entities, nouns, adj, verbs, adverbs:\n",
            "say: 475\n",
            "think: 129\n",
            "go: 129\n",
            "little: 127\n",
            "look: 105\n",
            "know: 103\n",
            "begin: 91\n",
            "come: 89\n",
            "thing: 79\n",
            "alice: 78\n",
            "\n",
            "10 least frequent terms with only named entities, nouns, adj, verbs, adverbs:\n",
            "lowing: 1\n",
            "cattle: 1\n",
            "lastly: 1\n",
            "grown: 1\n",
            "woman: 1\n",
            "riper: 1\n",
            "childhood: 1\n",
            "gather: 1\n",
            "joy: 1\n",
            "happy: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(C) - After reading more about spacy and the features that it has I decided to implement NER as the additional processing step since by using NER, it can extract important entities such as important names of people and places in the text \"Alice in Wonderland\" and these entities will end up making the list much more informative. Thus, I decided to keep only named entities and terms that are nouns, adjectives, verbs, and adverbs, since I believe will make a list that contains most words that are important to the subject matter of \"Alice in Wonderland\". After implementing this, I saw that the results were a shorter and much more informative list that can help identify what the text is much faster than the other lists through part 1."
      ],
      "metadata": {
        "id": "zGbz6U-Byip4"
      }
    }
  ]
}